{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole-v1 Demo\n",
    "\n",
    "## This activity was done using the DDQN method\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. \n",
    "- The system is controlled by applying a force of +1 or -1 to the cart. \n",
    "- The pendulum starts upright, and the goal is to prevent it from falling over. \n",
    "- A reward of +1 is provided for every timestep that the pole remains upright. \n",
    "- The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDQN Solver Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacqui.Muller\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DDQN_Solver():\n",
    "    def __init__(self, gamma, memory_size, min_memory_size, learning_rate_adam, HL_1_size, HL_2_size, batch_size,\n",
    "                 epsilon_all):\n",
    "        # Load important parameters\n",
    "        self.gamma = gamma  # discount rate\n",
    "        self.memory_size = memory_size  # size of the memory buffer\n",
    "        self.HL_1_size = HL_1_size  # number of nodes in the first hidden layer\n",
    "        self.HL_2_size = HL_2_size  # number of nodes in the second hidden layer\n",
    "        self.learning_rate_adam = learning_rate_adam  # learning rate for Adam optimizer\n",
    "        self.batch_size = batch_size  # batch size for training\n",
    "        self.min_memory_size = max(self.batch_size, min_memory_size)  # minimal memory size before we start training\n",
    "        self.epsilon_initial = epsilon_all['initial']  # epsilon-greedy policy - initial value\n",
    "        self.epsilon_decay = epsilon_all['decay']  # decay after each time step\n",
    "        self.epsilon_min = epsilon_all['min']  # minimal value of epsilon\n",
    "\n",
    "        # Initialize attributes\n",
    "        self.replay_buffer = deque()\n",
    "        self.global_step = 0  # counts the number of times we have trained our model = sum_{episode} timesteps_episode\n",
    "        self.most_recent_score = tf.Variable(0, dtype=tf.int32)  # most recent score - visualized in tensorboard\n",
    "        tf.summary.scalar('most_recent_score', self.most_recent_score)\n",
    "        self.epsilon = self.epsilon_initial  # we initialize our epsilon\n",
    "        self.epsilon_tensor = tf.Variable(self.epsilon, dtype=tf.float32)  # for tensorboard\n",
    "        tf.summary.scalar('epsilon', self.epsilon_tensor)\n",
    "\n",
    "        # Build online and target networks\n",
    "        self.__build_Q_net()\n",
    "\n",
    "        # Merge summaries\n",
    "        self.overall_summary = tf.summary.merge_all()\n",
    "\n",
    "        # Initialize variables and summary writer\n",
    "        self.__init_session()\n",
    "        self.summary_writer = tf.summary.FileWriter('/ddqn_summaries', self.session.graph)\n",
    "\n",
    "        # Synchronize Online and Target Network\n",
    "        self.update_target_network()\n",
    "\n",
    "    def __build_Q_net(self):\n",
    "        # Placeholders\n",
    "        self.input_state = tf.placeholder(tf.float32, [None, 4], 'Input_state')\n",
    "        self.input_action = tf.placeholder(tf.float32, [None, 2], 'Input_action')\n",
    "        self.target = tf.placeholder(tf.float32, [None], 'Target')\n",
    "\n",
    "        # Online Network Variables\n",
    "        self.W1_on = tf.Variable(tf.truncated_normal([4, self.HL_1_size]))\n",
    "        self.b1_on = tf.Variable(tf.constant(0.1, shape=[self.HL_1_size]))\n",
    "        self.HL_1_on = tf.nn.relu(tf.matmul(self.input_state, self.W1_on) + self.b1_on, )\n",
    "        self.W2_on = tf.Variable(tf.truncated_normal([self.HL_1_size, self.HL_2_size]))\n",
    "        self.b2_on = tf.Variable(tf.constant(0.1, shape=[self.HL_2_size]))\n",
    "        self.HL_2_on = tf.nn.relu(tf.matmul(self.HL_1_on, self.W2_on) + self.b2_on)\n",
    "        self.W3_on = tf.Variable(tf.truncated_normal([self.HL_1_size, 2]))\n",
    "        self.b3_on = tf.Variable(tf.constant(0.1, shape=[2]))\n",
    "        self.Q_ohr_on = tf.matmul(self.HL_2_on, self.W3_on) + self.b3_on\n",
    "\n",
    "        # Target Network Variables\n",
    "        self.W1_tn = tf.Variable(tf.truncated_normal([4, self.HL_1_size]))\n",
    "        self.b1_tn = tf.Variable(tf.constant(0.1, shape=[self.HL_1_size]))\n",
    "        self.HL_1_tn = tf.nn.relu(tf.matmul(self.input_state, self.W1_tn) + self.b1_tn, )\n",
    "        self.W2_tn = tf.Variable(tf.truncated_normal([self.HL_1_size, self.HL_2_size]))\n",
    "        self.b2_tn = tf.Variable(tf.constant(0.1, shape=[self.HL_2_size]))\n",
    "        self.HL_2_tn = tf.nn.relu(tf.matmul(self.HL_1_tn, self.W2_tn) + self.b2_tn)\n",
    "        self.W3_tn = tf.Variable(tf.truncated_normal([self.HL_1_size, 2]))\n",
    "        self.b3_tn = tf.Variable(tf.constant(0.1, shape=[2]))\n",
    "        self.Q_ohr_tn = tf.matmul(self.HL_2_tn, self.W3_tn) + self.b3_tn\n",
    "\n",
    "        # Q function and loss\n",
    "        self.Q_on = tf.reduce_sum(tf.multiply(self.Q_ohr_on, self.input_action), reduction_indices=1)\n",
    "\n",
    "        # Loss\n",
    "        self.loss = tf.reduce_mean(tf.square(self.target - self.Q_on), name='loss')\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "\n",
    "        # Train operations\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate_adam).minimize(self.loss)\n",
    "\n",
    "    def __init_session(self):\n",
    "        self.session = tf.InteractiveSession()\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" Samples a minibatch from the memory and based on it trains the network\n",
    "        \"\"\"\n",
    "        self.global_step += 1\n",
    "\n",
    "        # Just make sure that it breaks at the beginning when memory is not big enough < min_memory_size\n",
    "        if len(self.replay_buffer) < self.min_memory_size:\n",
    "            #print('The memory is too small to train')\n",
    "            return\n",
    "\n",
    "        # Sample from memory\n",
    "\n",
    "        mini_batch = random.sample(self.replay_buffer, self.batch_size)  # sampling without replacement\n",
    "        batch_s_old = [element[0] for element in mini_batch]\n",
    "        batch_a = [element[1] for element in mini_batch]\n",
    "        batch_r = [element[2] for element in mini_batch]\n",
    "        batch_s_new = [element[3] for element in mini_batch]\n",
    "        batch_d = [element[4] for element in mini_batch]\n",
    "\n",
    "        # Generating targets\n",
    "        Q_new_on = self.Q_ohr_on.eval(feed_dict={self.input_state: batch_s_new})  # forward pass - ONLINE NETWORK\n",
    "        Q_new_tn = self.Q_ohr_tn.eval(feed_dict={self.input_state: batch_s_new})  # forward pass - TARGET NETWORK\n",
    "        argmax = np.argmax(Q_new_on, axis=1)\n",
    "        Q_target = np.reshape(np.array([Q_new_tn[i][argmax[i]] for i in range(self.batch_size)]),\n",
    "                              newshape=self.batch_size)\n",
    "\n",
    "        # Generate targets\n",
    "        batch_target = []\n",
    "        for i in range(self.batch_size):\n",
    "            if batch_d[i]:\n",
    "                # The new state is the end game - its target Q value is definitely 0\n",
    "                batch_target.append(batch_r[i])\n",
    "            else:\n",
    "                batch_target.append(batch_r[i] + self.gamma * Q_target[i])\n",
    "\n",
    "        # Train and write summary\n",
    "        _, summary_str = self.session.run([self.train_op, self.overall_summary], feed_dict={\n",
    "            self.target: batch_target,\n",
    "            self.input_state: batch_s_old,\n",
    "            self.input_action: batch_a,\n",
    "        })\n",
    "        self.summary_writer.add_summary(summary_str, self.global_step)\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.__decay_epsilon()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        # We simply copy online network values into the target network\n",
    "        ops_list = []\n",
    "        ops_list.append(self.W1_tn.assign(self.W1_on))\n",
    "        ops_list.append(self.b1_tn.assign(self.b1_on))\n",
    "        ops_list.append(self.W2_tn.assign(self.W2_on))\n",
    "        ops_list.append(self.b2_tn.assign(self.b2_on))\n",
    "        ops_list.append(self.W3_tn.assign(self.W3_on))\n",
    "        ops_list.append(self.b3_tn.assign(self.b3_on))\n",
    "\n",
    "        self.session.run(ops_list)\n",
    "\n",
    "\n",
    "    def __decay_epsilon(self, printme=False):\n",
    "        \"\"\" Decays epsilon based on epsilon_decay\n",
    "        :param printme: print current value of epsilon\n",
    "        :type printme: bool\n",
    "        \"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        if printme:\n",
    "            print('The current value of epsilon is ' + str(self.epsilon))\n",
    "\n",
    "    def memorize(self, old_state, action, reward, new_state, done):\n",
    "        \"\"\" Inserts the most recent SARS and done into the memory - a is saved in the one hot representation \"\"\"\n",
    "        # Convert action to one hot representation\n",
    "        a_ohr = np.zeros(2)\n",
    "        a_ohr[action] = 1\n",
    "\n",
    "        # Make sure they have the right dimensions\n",
    "        old_state.shape = (4,)\n",
    "        a_ohr.shape = (2,)\n",
    "        new_state.shape = (4,)\n",
    "\n",
    "        # Add into replay_buffer and if necessary pop oldest memory\n",
    "        memory_element = tuple((old_state, a_ohr, reward, new_state, done))\n",
    "        self.replay_buffer.append(memory_element)\n",
    "        if len(self.replay_buffer) > self.memory_size:\n",
    "            self.replay_buffer.popleft()\n",
    "\n",
    "    def choose_action(self, old_state, policy_from_online):\n",
    "        \"\"\" Epsilon greedy policy \"\"\"\n",
    "        \n",
    "        # just a forward pass and max\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Explore\n",
    "            return np.random.choice([0, 1], 1)[0]\n",
    "        else:\n",
    "            # Exploit\n",
    "            old_state.shape = (1, 4)  # make sure it matches the placeholder shape (None, 4)\n",
    "            if policy_from_online:\n",
    "                return np.argmax(self.Q_ohr_on.eval(feed_dict={self.input_state: old_state}))\n",
    "            else:\n",
    "                return np.argmax(self.Q_ohr_tn.eval(feed_dict={self.input_state: old_state}))\n",
    "\n",
    "    def feed_most_recent_score(self, score):\n",
    "        \"\"\" Feeds the most recent score into solver class so that it can be visualizes in tensorboard together with epsilon\"\"\"\n",
    "        \n",
    "        option1 = self.most_recent_score.assign(score)\n",
    "        option2 = self.epsilon_tensor.assign(self.epsilon)\n",
    "        self.session.run([option1, option2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign Environment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_episodes = 2500\n",
    "render_bool = False\n",
    "penalize_bad_states = (True, -100)\n",
    "\n",
    "policy_from_online = True \n",
    "param_dict = {'gamma': 1,\n",
    "              'batch_size': 64,\n",
    "              'HL_1_size': 24,\n",
    "              'HL_2_size': 24,\n",
    "              'memory_size': 2000,\n",
    "              'min_memory_size': 100,\n",
    "              'learning_rate_adam': 0.001,\n",
    "              'epsilon_all': {'initial': 1, 'decay': 999 / 1000, 'min': 0.01}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Algorithm Termination Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_consecutive_episodes = 5\n",
    "threshold_average = 490"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f3c72aed534a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmy_solver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDDQN_Solver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparam_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CartPole-v1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msolved\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gym' is not defined"
     ]
    }
   ],
   "source": [
    "my_solver = DDQN_Solver(**param_dict)\n",
    "env = gym.make('CartPole-v1')\n",
    "solved = False\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Algorithm (Main Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "number_of_steps = []\n",
    "episodes = []\n",
    "\n",
    "for episode in range(number_of_episodes):\n",
    "    old_state = env.reset()\n",
    "    done = False\n",
    "    total = 0\n",
    "    while not done:\n",
    "        total += 1\n",
    "\n",
    "        if render_bool:\n",
    "            env.render()\n",
    "\n",
    "        # Choose action\n",
    "        action = my_solver.choose_action(old_state, policy_from_online)\n",
    "\n",
    "        # Take action\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Penalize\n",
    "        if penalize_bad_states[0] and done and total < 500:\n",
    "            reward = penalize_bad_states[1]\n",
    "\n",
    "        # Memorize\n",
    "        my_solver.memorize(old_state, action, reward, new_state, done and total < 500)\n",
    "\n",
    "        # Train\n",
    "        if not solved:\n",
    "            my_solver.train()\n",
    "\n",
    "        # Move forward\n",
    "        old_state = new_state\n",
    "\n",
    "    # Append results and check if solved\n",
    "    results.append(total)\n",
    "\n",
    "    if np.mean(results[-min(number_of_consecutive_episodes, episode):]) > threshold_average:\n",
    "        solved = True\n",
    "        print('Stable solution found - no more training required!')\n",
    "        break\n",
    "    else:\n",
    "        solved = False\n",
    "        \n",
    "    episodes.append(episode)\n",
    "    number_of_steps.append(total)\n",
    "\n",
    "    print('The episode %s lasted for %s steps' % (episode, total))\n",
    "    my_solver.feed_most_recent_score(total)\n",
    "    my_solver.update_target_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Statistics into a Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame()\n",
    "\n",
    "stats_df['episode'] = episodes\n",
    "stats_df['steps'] = number_of_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.plot()\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"cart pole.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('<img src=\"cart pole.gif\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
